{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f7c865-f756-48b9-a9c0-96d9afb4b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4630dab9-74a7-427f-8a40-7a134d26f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = str(Path.cwd().parents[0])\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcc0bd8-f739-4180-9383-76dba48d3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from quovadis_tad.dataset_utils.dataset_reader import datasets\n",
    "from quovadis_tad.dataset_utils.data_utils import preprocess_data, normalise_scores\n",
    "from quovadis_tad.evaluation.single_series_evaluation import evaluate_ts\n",
    "from quovadis_tad.model_utils.model_def import test_embedder\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09af8976-f858-4273-86db-487d42ccb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0: \n",
    "   tf.config.experimental.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d8f1d-d0ff-4863-9cdc-e14bf99b95c2",
   "metadata": {},
   "source": [
    "# Train NN-Baselines Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4039be-50eb-4205-a42f-edcff3a152dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We provide four model configurations to train corresponding to the introduced four NN-Baselines. See the \"src/model_configs\" folder. To train these on a dataset, go to project root and run from the console the following command by providing the dataset name. This will train all model configs on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e848523-47c8-45d2-bf91-a72551aad6b9",
   "metadata": {},
   "source": [
    "`CUDA_VISIBLE_DEVICES=0 python ./src/run_all_configs.py wadi_112`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b9123-4971-4d3b-bf08-3817022b6566",
   "metadata": {},
   "source": [
    "#### or pass a specific config name to train with --config-to-run argument as below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617011e-a280-462b-a85a-4a1ff0713c84",
   "metadata": {},
   "source": [
    "`CUDA_VISIBLE_DEVICES=0 python ./src/run_all_configs.py wadi_112 --config-to-run gcn_lstm_model_seq_5.yaml`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c6f4e-f9eb-493c-bdcd-8f87e6fa577d",
   "metadata": {},
   "source": [
    "#### See the file run_all_configs for the input arguments options. The trained model checkpoints will be saved by default to model_configs folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147c9a3-5d72-4f60-a845-03c4a48c9935",
   "metadata": {},
   "source": [
    "# Trained Model Inference & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a7514-d7aa-48cd-91fa-f2a5025f764e",
   "metadata": {},
   "source": [
    "### We include our trained model checkpoints for SWAT and WADI datasets in the model_checkpoints folder. These or the ones you train can be tested as below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329436e-c9ea-4f74-af64-2f73dc0454ab",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89cde3d3-1ee0-43c6-b8a9-619f1ebd17f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 27\n",
      "Number of test files: 27\n",
      "Number of label files: 27\n",
      " [WARNING:] Using first trace of this dataset. Specify dataset_trace otherwise\n",
      "NN-Baseline: 1_Layer_MLP\n",
      "Loaded pretrained_checkpoint\n"
     ]
    }
   ],
   "source": [
    "pred, gt = test_embedder(module_path,\n",
    "                                 'ourBench',                           # Dataset name one of e.g 'swat', 'wadi_127', 'wadi_112', 'smd', see dataset_reader enum\n",
    "                                 dataset_trace=None,\n",
    "                                 model_name='1_Layer_MLP',    # one of the NN-Baselines '1_Layer_MLP', 'Single_block_MLPMixer', 'Single_Transformer_block', '1_Layer_GCN_LSTM'\n",
    "                                 load_weights=True,\n",
    "                                 training=False,\n",
    "                                 subset='test'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce23e2-9d88-4146-82ca-3773361d9085",
   "metadata": {},
   "source": [
    "## Evaluate the model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18680d6-9418-4abd-8c7c-d464096476cc",
   "metadata": {},
   "source": [
    "### Evaluate the prediction under Point-Wise metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f053e2-18a3-4359-8044-80f0b9c708fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res, df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalise_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoint_wise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\quovadis\\QuoVadisTAD\\quovadis_tad\\evaluation\\single_series_evaluation.py:18\u001b[0m, in \u001b[0;36mevaluate_ts\u001b[1;34m(scores, targets, eval_method, verbose)\u001b[0m\n\u001b[0;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith_PA\u001b[39m\u001b[38;5;124m'\u001b[39m: eval_method})\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m    results, df \u001b[38;5;241m=\u001b[39m \u001b[43mget_ts_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\quovadis\\QuoVadisTAD\\quovadis_tad\\evaluation\\single_series_evaluation.py:31\u001b[0m, in \u001b[0;36mget_ts_eval\u001b[1;34m(scores, targets, eval_method, verbose)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#targets = torch.from_numpy(targets)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#scores = torch.from_numpy(scores)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint_wise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# point wise standard metrics\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mts_evalator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_f1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m eval_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange_wise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# recall-consistant [wagner et al. 2023]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     results \u001b[38;5;241m=\u001b[39m ts_evalator\u001b[38;5;241m.\u001b[39mbest_ts_f1_score(targets, scores)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\quovadis\\QuoVadisTAD\\quovadis_tad\\evaluation\\scoring_functions.py:327\u001b[0m, in \u001b[0;36mEvaluator.best_f1_score\u001b[1;34m(self, labels, scores)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_f1_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, labels: np\u001b[38;5;241m.\u001b[39mndarray, scores: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    Compute the classic point-wise :math:`F_{1}` score.\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m        produced the maximal score.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_fbeta_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\quovadis\\QuoVadisTAD\\quovadis_tad\\evaluation\\scoring_functions.py:285\u001b[0m, in \u001b[0;36mEvaluator.best_fbeta_score\u001b[1;34m(self, labels, scores, beta)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_fbeta_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, labels: np\u001b[38;5;241m.\u001b[39mndarray, scores: np\u001b[38;5;241m.\u001b[39mndarray, beta: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    Compute the classic point-wise F_beta score.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     precision, recall, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     numerator \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m precision \u001b[38;5;241m*\u001b[39m recall\n\u001b[0;32m    288\u001b[0m     denominator \u001b[38;5;241m=\u001b[39m beta \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m precision \u001b[38;5;241m+\u001b[39m recall\n",
      "File \u001b[1;32m~\\Desktop\\miniconda3\\envs\\quovadis\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\miniconda3\\envs\\quovadis\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:965\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[1;34m(y_true, probas_pred, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    866\u001b[0m     {\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m     y_true, probas_pred, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    877\u001b[0m ):\n\u001b[0;32m    878\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;124;03m    array([0.1 , 0.35, 0.4 , 0.8 ])\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 965\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobas_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;66;03m# Drop thresholds corresponding to points where true positives (tps)\u001b[39;00m\n\u001b[0;32m    971\u001b[0m         \u001b[38;5;66;03m# do not change from the previous or subsequent point. This will keep\u001b[39;00m\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;66;03m# only the first and last point for each tps value. All points\u001b[39;00m\n\u001b[0;32m    973\u001b[0m         \u001b[38;5;66;03m# with the same tps value have the same recall and thus x coordinate.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;66;03m# They appear as a vertical line on the plot.\u001b[39;00m\n\u001b[0;32m    975\u001b[0m         optimal_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m    976\u001b[0m             np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    977\u001b[0m                 [[\u001b[38;5;28;01mTrue\u001b[39;00m], np\u001b[38;5;241m.\u001b[39mlogical_or(np\u001b[38;5;241m.\u001b[39mdiff(tps[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), np\u001b[38;5;241m.\u001b[39mdiff(tps[\u001b[38;5;241m1\u001b[39m:])), [\u001b[38;5;28;01mTrue\u001b[39;00m]]\n\u001b[0;32m    978\u001b[0m             )\n\u001b[0;32m    979\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\miniconda3\\envs\\quovadis\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:817\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    815\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    819\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    820\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: multilabel-indicator format is not supported"
     ]
    }
   ],
   "source": [
    "res, df = evaluate_ts(normalise_scores(pred).max(1),\n",
    "                   gt,\n",
    "                   eval_method='point_wise',\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445f630-98e0-4e95-8c64-c7a39559f4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d40b9c2-1db8-4201-a39c-cc2c2cc39864",
   "metadata": {},
   "source": [
    "### Evaluate the prediction under Range-Wise metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46501f71-585a-4a1e-80ae-634c1c76eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           range_wise\n",
      "       F1       0.758\n",
      "Precision       0.610\n",
      "   Recall       1.000\n",
      "    AUPRC       0.619\n",
      "    AUROC       0.495\n"
     ]
    }
   ],
   "source": [
    "res, df = evaluate_ts(normalise_scores(pred).max(1),\n",
    "                   gt,\n",
    "                   eval_method='range_wise',\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2a63c-b2f1-4901-b920-b7855199185a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
